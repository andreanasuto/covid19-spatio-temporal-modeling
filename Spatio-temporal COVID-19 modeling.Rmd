---
title: "UK COVID19 Spread Modeling across space and time"
output: html_notebook
---
## Introduction
On March 11th 2020, the outbreak caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) was declared a global pandemic by the WHO (WHO, 2020). In the UK, the number of cases, whatever undetected or not present, stayed largely low until mid March. More than 14 months later, 4,433,000 British people tested positive for COVID-19 and 127,603 lost their life (last update: 9 May 2021, NYT/Public Health Office). This study focuses on designing models across different technical and theoretical approaches to analyze, discuss and potentially predict the COVID-19 infection in the UK, focusing on the nonspatial stationarity of the variables i.e. the fact that a dependent variable and a set of predictors may vary across geographical space.
The dataset aggregates COVID-19 cases elaborated by public sources containing information about the tested positive cases, census data and IMD score across Upper Tear Location Authorities (UTLA) between 31/01/2020 and 05/02/2021.

```{r message=FALSE, warning=FALSE}
# Data manipulation, transformation and visualisation
library(tidyverse)
# Nice tables
library(kableExtra)
# Create multiple plots
library(patchwork)
# Simple features (a standardised way to encode vector data ie. points, lines, polygons)
library(sf) 
# Spatial objects conversion
library(sp) 
# Thematic maps
library(tmap) 
# Colour palettes
library(RColorBrewer) 
# More colour palettes
library(viridis) # nice colour schemes
# Fitting multilevel models
library(lme4)
# Tools for extracting information generated by lme4
library(merTools)
# Exportable regression tables
library(jtools)
library(stargazer)
library(sjPlot)
# Fitting geographically weighted regression models
library(spgwr)
# Assess multicollinearity
library(car)
# Obtain correlation coefficients
library(corrplot)
# Highlight data on plots
library(gghighlight)
# Analysing spatio-temporal data
#library(STRbook)
library(spacetime)
# Date parsing and manipulation
library(lubridate)
# Applied statistics
library(MASS)
# Statistical tests for linear regression models
library(lmtest)
# Fit spatial random effects models
library(FRK)
```

```{r include=FALSE}
covid_19 <- st_read('/Users/Andrea/OneDrive\ -\ The\ University\ of\ Liverpool/Spatial\ Analysis\ and\ Modeling/data/assignment_2_covid/covid19_eng.gpkg')
```
## Part I
**Methods**
The cumulative cases and rate of infections by 100,000 people are calculated in the total time range available. Three predictors are selected: total working population remotely working from home, the households living in crowded housing and the residents part of a black or ethnic minority group (BME). A new dataframe is created with the selected variables and the proportional values across areas. The percentage of people working from home might be underestimated since the total residents' count includes those that are not of working age. A first multi-level model is built with only the intercept varying across regions. In addition to the varying intercept, the second multi-level model adds three potential individual-level predictors. The variables are chosen based on the existing literature backing the role of the remote working (Robinson et al., 2020), the crowded housing conditions (Tinson and Clair, 2020) and the presence of black or minority ethnic groups i.e. BME (Green et al., 2021) as key determinants in the spread of COVID-19. Lastly, a Variance Partition Component (VPC) analysis is conducted to compare the two models.

**Results and interpretation** 
In model one, only the intercept varies by region. The summary table of the model shows the estimated standard errors (SE) both for the intercept and the residuals. According to this model, on average within regions, the SE of the infection rate deviates by 1,164 while the unexplained component of this variability equals 1,598. In model one the fixed effects (FEs) for the intercept simply represent the average value of the cases across regions (6436.4). In model two (Table Two) the estimated SE is smaller both for the intercept and the residual part compared to model one i.e. there is less variation in the number of cases across regions in the UK. In multi-level modeling, the FEs are the standard linear regression coefficients and their interpretation is the usual. A 1% increase in the percentage of people working from home and living in crowded housing decrease the infection rate by -111,984.65 and -11,431.52 respectively while an increase in the BME would increase the rate by 5,685.29. As shown in previous research, BME groups tend to be disproportionately represented in the so-called key workers i.e. workers providing essential services during the lockdowns (The Health Foundation, 2020). As result, they have less possibility to shield themselves from risky face-to-face contacts. They have a higher percentage of positive cases out of their total COVID-19 tested population compared to other ethnic groups, probably because they are tested less (Green et al., 2021). 
Crowded housing is another key element in the COVID-19 spread (Tinson and Clair, 2020). This is not confirmed in the model. However, while all the variables seem statistically significant given their low p-value < 0.001, the crowded housing feature is not (p-value = 0.24). Working from home is a key strategy to reduce mobility and face-to-face interactions. Mobility, in general, seems to be one of the most important elements to assess differences within areas and ethnic groups in terms of deaths and COVID-19 infections (Chang et al., 2020). In these two models, the VPC is the correlation of randomly picked values of the rate of infection (the dependent variable) between regions. In model one is 0.35 and in model roughly the same (0.36). This means that in both cases roughly 35% of the variability is explained between groups (UK regions) while 65% by variation within each region. Model two with individual predictors has a better total pseudo R-Squared (0.75) compared to the only intercept-varying model one (0.35), the AIC score is also lower than model one.

```{r warning=FALSE, message=FALSE}
# Create a columns with all the cumulative COVID 19 cases across the time range
# define columns where to calculate sum of the cases
days <- covid_19[,c(9:380)]
# set geometry to none
days <- st_set_geometry(days, NULL)
# calculate cumulative cases
covid_19$cum_cases <- rowSums(days)
# calculate rate of infections per 100,000 people
covid_19$r_infections <- (covid_19$cum_cases/covid_19$Residents) * 100000
```

```{r, results = FALSE, warning=FALSE}
# obtain a matrix of Pearson correlation coefficients
df_sel <- st_set_geometry(covid_19[,9:138], NULL) # temporary data set removing geometries
corr_vector <- cor(df_sel, use="complete.obs",df_sel$r_infections, method="pearson")
```
```{r}
# Select the variables transform them in proportion
covid_19 <- covid_19 %>% mutate(
  crowded_hou_r = Crowded_housing / Households, # share of crowded housing
  work_from_home_r = (Work_from_home) / Residents, # share of resident working from home
  bme = (Mixed + Indian + Pakistani + Bangladeshi + Chinese + Other_Asian + Black + Other_ethnicity) / Residents, # share of black and minority ethnic residents
  young = (Age_20_to_29) / Residents
)
# Create a new dataframe with the variables to study
df <- covid_19  %>%  dplyr::select(objectid,
                         bng_n,
                         long,
                         lat,
                         st_areasha,
                         ctyua19cd, 
                         ctyua19nm, 
                         Region, 
                         r_infections, 
                         crowded_hou_r,
                         work_from_home_r,
                         bme,
                         young,
                         Residents)
```

```{r}
# 1) Fit a varying-intercept model with no explanatory variables. Let the intercept to vary by region.
# specify a model equation
eq1 <- r_infections ~ 1 + (1 | Region)
m1 <- lmer(eq1, data = df)
```

```{r}
## 2) Fit a varying-intercept model with including at least three explanatory variables.
eq2 <- r_infections ~ 1 +  work_from_home_r +  bme  + crowded_hou_r + (1 | Region)
m2 <- lmer(eq2, data = df)
```

```{r, results=FALSE,  include=FALSE}
# compute the Variance Partition Coefficient (VPC) for the models estimated according to points 1 and 2 above.
# Model One VPC
summ(m1)
```
```{r results=FALSE,  include=FALSE}
# Model Two VPC
summ(m2)
```

```{r}
# create caterpillar plots
caterpillar_1 <- plotREsim(REsim(m1)) + labs(title = "Model One")
caterpillar_2 <- plotREsim(REsim(m2)) + labs(title = "Model Two")

# plot them and add main title
(caterpillar_1 + caterpillar_2) + 
  plot_annotation(title = "Figure 1 - Caterpillar Plots Random Effects") 
```

## Part II
**Methods** 
A first model (model three) is built using the same variables as in Part I. The variable that has a varying slope by region is the percentage of total residents working from home. This number varies vastly across professions (Bartik et al., 2020) and as result, it might also vary by areas based on the concentration of the professions allowing remote working (Felstead and Reuschke, 2020). A second model (model four) is built allowing the same aforementioned variable to vary both by intercept and slope. Lastly, an ANOVA analysis is run on both models to compare them.

**Results and interpretation**
Model four has the slope of the ‘Work From Home’ variable to vary across regions. On average across areas, the standard error of the variable’s coefficient is 21,985 - i.e. how much it deviates from model national average at the regional level - and the unexplained variability of the intercept within regions has an estimated standard error of 1,037 - i.e. how areas inside the region on average deviates from the regional average. While these results might be particularly interesting, they are statistically significant only in two regions (Fig. 1). The FEs have similar interpretations and limitations as in model two (Table Two). A 1% increase in people working from home and residents in crowded housing will reduce the number of total cases by 115,571.8 and 7243.4 respectively while the BME predictor will increase the total cases per 100,000 people by 5743.4 for the same increment. Model four further extends model three allowing the intercept of the remotely working residents to vary across regions as well as the slope. As in model three (Table Two), the average difference in the slope across regions is 28,727 (SE) while the intercept is 1,336. The unexplained component that varies within regions equals 927. Interestingly, as the intercepts regions tend to be higher, the coefficients are almost 1:1 proportionally lower since the correlation is -0.94. This might suggest that in areas where cases are consistently higher on average, an increase in the percentage of people working remotely might not be beneficial as in other regions since the coefficient will be low in value. 
The estimate of the intercept variation is not statistically significant across any regions while in the case of the slope is statistically significant in four cases out of nine (Fig 1). When it comes to interpreting the fixed effects, the results and interpretation are similar to previous models. Lastly, using the ANOVA analysis, model four seems a better fit given a lower AIC score compared to model three.

```{r}
# specify a model equation and remove the varying intercept from 'Work From Home' variable
eq3 <- r_infections ~ work_from_home_r + bme + crowded_hou_r + (work_from_home_r -1 | Region)
# run the model based on the df and equation
m3 <- lmer(eq3, data = df)
```

```{r results=FALSE, include=FALSE}
# plot residuals
plotREsim(REsim(m3)) 
```

```{r}
## 2. Fit a varying-intercept and varying-slope model
# specify a model equation
eq4 <- r_infections ~  work_from_home_r + bme + crowded_hou_r + (1 +  work_from_home_r | Region)
m4 <- lmer(eq4, data = df)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# plot residuals
plotREsim(REsim(m4)) 
```


```{r message=FALSE, warning=FALSE, include=FALSE, echo=TRUE}
# run ANOVA analysis
anova(m3, m4)
```

## Part III
**Methods** 
Two geographically weighted regression (GWR) models are built using two different approaches. A new standard linear regression model (model five) is set using the same variables since model two. A variance inflation factor (VIF) test is conducted to assess potential multicollinearity. Through a cross-validation technique, a fixed and adaptive bandwidth are calculated. Based on them, two GWR models are fitted and the R-Squared are calculated across the different UTLAs displaying their values across areas with two maps, joint by a third map displaying the differences in the R-squared between the two models (Fig. 2). To assess the statistical significance, the t-students are calculated and an arbitrary benchmark is set to 2 to classify the predictors’ significance. The results are then displayed in six maps (Fig. 2)

**Results and interpretation** 
The standard regression model explains 66% of the total variability of the rate of infections (Adjusted R-Squared value) and it might not have a major issue with multicollinearity in the predictors; all the VIF scores are <10. (Belsley et al., 2015.) The fixed bandwidth has a value of approximately 40km while the adaptive is 20m. The Quasi-Global R-squared for model five (fixed bandwidth) is 0.89 while for model six (adaptive bandwidth) is 0.92. The interpretation of the coefficients is similar among the two models. The associations drastically change across areas. In model five (Table One), a 1% increase in the number of remote workers reduces the cases (between 238,675 and 89,729 less) but it might also increase them by 5,971 (median = -119,459). An increase of 1% of BME residents might grow the cases by 38,868 as well as decrease them by 4,502 (median = 4871). Lastly, the ‘crowded housing’ has a median value that decreases the total number of cases by 3,755 but it might increase them in a range between 8,805 or 188,984 based on the areas. Similar patterns and interpretations are seen in model six. However, it is worth noticing how the ‘work from home’ feature has a consistently negative coefficient compared to model five across UTLAs.
The statistical significance of the predictors in the models vastly changes across predictors and areas (Figure Two). The ‘Work from Home’ is mostly significant in both models with some exceptions in Lincolnshire and Cornwall regions for the fixed bandwidth model. The majority of the results for the BME are statistically insignificant in both models and for ‘crowded housing’ just a tiny percentage of the total UTLAs are significant. As results, the interpretation of the coefficients needs caution. The two bandwidths might both improve and reduce the R-squared between areas. For example, North East Lincolnshire and East Yorkshire have a lower R-squared with the adaptive kernel. However, in the South East, Norfolk and North East, it is higher than with the fixed band. Model five’s bandwidth looks at 40% of its nearest UTLAs neighbors from a given point while the adaptive bandwidth only 2%. This element might explain the difference in predictability performances.

```{r, results=FALSE,  include=FALSE}
reg_shp <- st_read('/Users/Andrea/OneDrive\ -\ The\ University\ of\ Liverpool/Spatial\ Analysis\ and\ Modeling/data/gwr/Regions_December_2019_Boundaries_EN_BGC.shp')
```

```{r message=FALSE, warning=FALSE, include=FALSE, echo=TRUE}
# specify a model equation
eq5 <- r_infections ~ work_from_home_r + bme + crowded_hou_r
m5 <- lm(formula = eq5, data = df)

# estimates
summary(m5)
```
```{r, results=FALSE,  include=FALSE}
vif(m5)
```

**Fixed bandwidth**
```{r message=FALSE, warning=FALSE, results=FALSE}
# attach dataframe
attach(df)

# find optimal kernel bandwidth using cross validation
fbw <- gwr.sel(eq5, 
               data = df, 
               coords=cbind( long, lat),
               longlat = TRUE,
               adapt=FALSE, 
               gweight = gwr.Gauss, 
               verbose = FALSE)

# view selected bandwidth
fbw
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# fit a gwr based on fixed bandwidth
fb_gwr <- gwr(eq5, 
            data = df,
            coords=cbind( long, lat),
            longlat = TRUE,
            bandwidth = fbw, 
            gweight = gwr.Gauss,
            hatmatrix=TRUE, 
            se.fit=TRUE)

# plot summary of the model - Table One
fb_gwr
```

```{r}
# write gwr output into a data frame
fb_gwr_out <- as.data.frame(fb_gwr$SDF)
df$fmb_localR2 <- fb_gwr_out$localR2
```

```{r, results=FALSE, echo=TRUE,  include=FALSE}
# Map Local R2 using a fixed bandwidth
legend_title = expression("Fixed: Local R2") # set title
# create map
map_fbgwr1 = tm_shape(df) +
  tm_fill(col = "fmb_localR2", title = legend_title, palette = 'RdBu', style = "cont") + # add fill
  tm_borders(col = "white", lwd = .1)  + # add borders
  tm_compass(type = "arrow", position = c("right", "top") , size = 0.5) + # add compass
  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c("center", "bottom")) + # add scale bar
  tm_layout(bg.color = "white") # change background colour
  map_fbgwr1 + tm_shape(reg_shp) + # add region boundaries
  tm_borders(col = "white", lwd = .5) # add borders
```
**Statistical significance across variables for model five**
```{r message=FALSE, warning=FALSE, results=FALSE,  include=FALSE}
# compute t statistic for Work From Home in model five with the fixed bandwidth
df$t_work_from_home_r = fb_gwr_out$work_from_home_r / fb_gwr_out$work_from_home_r_se

# categorise t values
df$t_wfh_cat <- cut(df$t_work_from_home_r,
                     breaks=c(min(df$t_work_from_home_r), -2, 2, max(df$t_work_from_home_r)),
                     labels=c("sig","nonsig", "sig"))

# map statistically significant coefs for Work From Home
legend_title = expression("M5 - Work From Home: significant")
map_sig_wfh_fb = tm_shape(df) + 
  tm_fill(col = "t_wfh_cat", title = legend_title, legend.hist = TRUE,  textNA = "", colorNA = "white") +  # add fill
  tm_borders(col = "white", lwd = .1)  + # add borders
  tm_compass(type = "arrow", position = c("right", "top") , size = 0.5) + # add compass
  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c("center", "bottom")) + # add scale bar
  tm_layout(bg.color = "white", legend.outside = TRUE) # change background colour & place legend outside

map_sig_wfh_fb + tm_shape(reg_shp) + # add region boundaries
  tm_borders(col = "white", lwd = .5) # add borders
```

```{r, results=FALSE, messages=FALSE,  include=FALSE}
# compute t statistic for BME in model five with the fixed bandwidth
df$t_bme = fb_gwr_out$bme / fb_gwr_out$bme_se

# categorise t values
df$t_bme_cat <- cut(df$t_bme,
                     breaks=c(min(df$t_bme), -2, 2, max(df$t_bme)),
                     labels=c("sig","nonsig", "sig"))

# map statistically significant coefs for Black and Ethnic Minority Groups
legend_title = expression("M5 - BME: significant")
map_sig_bme_fb = tm_shape(df) + 
  tm_fill(col = "t_bme_cat", title = legend_title, legend.hist = TRUE,  textNA = "", colorNA = "white") +  # add fill
  tm_borders(col = "white", lwd = .1)  + # add borders
  tm_compass(type = "arrow", position = c("right", "top") , size = 0.5) + # add compass
  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c("center", "bottom")) + # add scale bar
  tm_layout(bg.color = "white", legend.outside = TRUE) # change background colour & place legend outside

map_sig_bme_fb + tm_shape(reg_shp) + # add region boundaries
  tm_borders(col = "white", lwd = .5) # add borders
```

```{r, results=FALSE,  include=FALSE}
# compute t statistic for Crowded Housing in model five with the fixed bandwidth
df$t_ch = fb_gwr_out$crowded_hou_r / fb_gwr_out$crowded_hou_r_se

# categorise t values
df$t_ch_cat <- cut(df$t_ch,
                     breaks=c(min(df$t_ch), -2, 2, max(df$t_ch)),
                     labels=c("sig","nonsig", "sig"))

# map statistically significant coefs for Crowded Housing
legend_title = expression("M5 - Crowded Housing: significant")
map_sig_ch_fb = tm_shape(df) + 
  tm_fill(col = "t_ch_cat", title = legend_title, legend.hist = TRUE,  textNA = "", colorNA = "white") +  # add fill
  tm_borders(col = "white", lwd = .1)  + # add borders
  tm_compass(type = "arrow", position = c("right", "top") , size = 0.5) + # add compass
  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c("center", "bottom")) + # add scale bar
  tm_layout(bg.color = "white", legend.outside = TRUE) # change background colour & place legend outside
map_sig_ch_fb + tm_shape(reg_shp) + # add region boundaries
  tm_borders(col = "white", lwd = .5) # add borders
```

**Adaptive bandwidth - Model Six**
```{r}
# find optimal kernel bandwidth using cross validation
abw <- gwr.sel(eq5, 
               data = df, 
               coords=cbind( long, lat),
               longlat = TRUE,
               adapt = TRUE, 
               gweight = gwr.Gauss, 
               verbose = FALSE)

# view selected bandwidth
abw
```

```{r warning=FALSE, include=FALSE}
# fit a gwr based on adaptive bandwidth
ab_gwr <- gwr(eq5, 
            data = df,
            coords=cbind( long, lat),
            longlat = TRUE,
            adapt = abw, 
            gweight = gwr.Gauss,
            hatmatrix=TRUE, 
            se.fit=TRUE)

ab_gwr
```

```{r,include=FALSE}
# write gwr output into a data frame
ab_gwr_out <- as.data.frame(ab_gwr$SDF)
df$abw_R2 <- ab_gwr_out$localR2

# Map Local R2 using an adaptive bandwidth
legend_title = expression("Adaptive: Local R2")
map_abgwr1 = tm_shape(df) +
  tm_fill(col = "abw_R2", title = legend_title, palette = 'RdBu', style = "cont") + # add fill
  tm_borders(col = "white", lwd = .1)  + # add borders
  tm_compass(type = "arrow", position = c("right", "top") , size = 0.5) + # add compass
  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c("center", "bottom")) + # add scale bar
  tm_layout(bg.color = "white") # change background colour
  map_abgwr1 + tm_shape(reg_shp) + # add region boundaries
  tm_borders(col = "white", lwd = .5) # add borders
```

**Statistical significance across variables**
```{r, results=FALSE,  include=FALSE}
# compute t statistic for Work From Home in model six with the adaptive bandwidth
df$t_work_from_home_r = ab_gwr_out$work_from_home_r / ab_gwr_out$work_from_home_r_se

# categorise t values
df$t_wfh_cat <- cut(df$t_work_from_home_r,
                     breaks=c(min(df$t_work_from_home_r), -2, 2, max(df$t_work_from_home_r)),
                     labels=c("sig","nonsig", "sig"))

# map statistically significant coefs for Work From Home
legend_title = expression("M6 - Work From Home: significant")
map_sig_wfh_ab = tm_shape(df) + 
  tm_fill(col = "t_wfh_cat", title = legend_title, legend.hist = TRUE,  textNA = "", colorNA = "white") +  # add fill
  tm_borders(col = "white", lwd = .1)  + # add borders
  tm_compass(type = "arrow", position = c("right", "top") , size = 0.5) + # add compass
  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c("center", "bottom")) + # add scale bar
  tm_layout(bg.color = "white", legend.outside = TRUE) # change background colour & place legend outside

map_sig_wfh_ab + tm_shape(reg_shp) + # add region boundaries
  tm_borders(col = "white", lwd = .5) # add borders
```

```{r, results=FALSE,  include=FALSE}
# compute t statistic for BME in model six with the adaptive bandwidth
df$t_bme_ab = ab_gwr_out$bme / ab_gwr_out$bme_se

# categorise t values
df$t_bme_ab_cat <- cut(df$t_bme_ab,
                     breaks=c(min(df$t_bme_ab), -2, 2, max(df$t_bme_ab)),
                     labels=c("sig","nonsig", "sig"))

# map statistically significant coefs for Black and Ethnic Minority Groups
legend_title = expression("M6 - BME: significant")
map_sig_bme_ab = tm_shape(df) + 
  tm_fill(col = "t_bme_ab_cat", title = legend_title, legend.hist = TRUE,  textNA = "", colorNA = "white") +  # add fill
  tm_borders(col = "white", lwd = .1)  + # add borders
  tm_compass(type = "arrow", position = c("right", "top") , size = 0.5) + # add compass
  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c("center", "bottom")) + # add scale bar
  tm_layout(bg.color = "white", legend.outside = TRUE) # change background colour & place legend outside

map_sig_bme_ab + tm_shape(reg_shp) + # add region boundaries
  tm_borders(col = "white", lwd = .5) # add borders
```

```{r, results=FALSE,  include=FALSE}
# compute t statistic for Crowded Housing in model six with the adaptive bandwidth
df$t_ch_ab = ab_gwr_out$crowded_hou_r / ab_gwr_out$crowded_hou_r_se

# categorise t values
df$t_ch_ab_cat <- cut(df$t_ch_ab,
                     breaks=c(min(df$t_ch_ab), -2, 2, max(df$t_ch_ab)),
                     labels=c("sig","nonsig", "sig"))

# map statistically significant coefs for Crowded Housing
legend_title = expression("M6 - Crowded Housing: significant")
map_sig_ch_ab = tm_shape(df) + 
  tm_fill(col = "t_ch_ab_cat", title = legend_title, legend.hist = TRUE,  textNA = "", colorNA = "white") +  # add fill
  tm_borders(col = "white", lwd = .1)  + # add borders
  tm_compass(type = "arrow", position = c("right", "top") , size = 0.5) + # add compass
  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c("center", "bottom")) + # add scale bar
  tm_layout(bg.color = "white", legend.outside = TRUE) # change background colour & place legend outside

map_sig_ch_ab + tm_shape(reg_shp) + # add region boundaries
  tm_borders(col = "white", lwd = .5) # add borders
```

```{r results=FALSE}
# define a new variable equals to the difference in R-Squared results
df$bandwidth_difference_R2 <- (df$abw_R2 - df$fmb_localR2)
```
```{r, results=FALSE,  include=FALSE}
# Map the gains and loses in R-Squared value between fixed and adaptive bandwidth
legend_title = expression("Difference Adaptive - Fixed R2")
map_diff = tm_shape(df) +
  tm_fill(col = "bandwidth_difference_R2", # add fill
          title = legend_title,            # include title
          palette = 'RdBu',              # set palette
          style = "cont",                  # set continuous style
          midpoint = NA) +                 # set midpoint to NA since it includes negative and positive values
  tm_borders(col = "white", lwd = .1)  + # add borders
  tm_compass(type = "arrow", position = c("right", "top") , size = 0.5) + # add compass
  tm_scale_bar(breaks = c(0,1,2), text.size = 0.7, position =  c("center", "bottom")) + # add scale bar
  tm_layout(bg.color = "white") # change background colour
  map_diff + tm_shape(reg_shp) + # add region boundaries
  tm_borders(col = "white", lwd = .5) # add borders
```


```{r, message=FALSE}
# Plot Fixed and Adaptive Bandwidth map with R-Squared and statistical significance
tmap_arrange(map_fbgwr1,map_abgwr1, map_diff,map_sig_wfh_fb, map_sig_bme_fb, map_sig_ch_fb, map_sig_wfh_ab, map_sig_bme_ab, map_sig_ch_ab, ncol=3)
```


## Part IV
**Methods** 
The spread of a virus is a chain-linked event placed in time and space. A person gets infected by another person across a certain time. The serial interval is “the time interval for which the infector and infectee show the symptoms” and it is an essential epidemiological modeling premise (Rai et al, 2021). The COVID-19 serial interval estimate is slightly higher than 5 days (Rai et al., 2021), as result, the cases are grouped by this time interval and multiple maps are built to show the variation of the infection rate across time.
Once a timestamp is defined, a spatio-temporal (ST) model is built to study how the infection rate evolves across space and time in the UK. Its results are compared to GWR models. To visualize the temporal variations a Hovmöller Plots is created with UTLAs larger than 300,000 citizens (Fig 3).

**Results and interpretation**
Starting from mid-March, COVID-19 cases started to emerge in the UK. As Fig 3 shows key areas were in major cities as London (as Hamlets Tower, Redbridge), Manchester and Liverpool. The ‘three waves’ structure is fairly visible, particularly the difference in the intensity between a new spread around October 2020 and Jan 2021 (Fig 3). During October, firstly and mostly the areas of Nottingham, Liverpool and Manchester were hit and while London area in January. Both GWR models have a better R-squared score compared to the ST model (adjusted R-squared=0.37). However, while in the previous two models, the ‘crowded housing’ and ‘BME’ predictors are largely statistically not significant, in the ST models they are. The percentage of people working from home and living in crowded housing is still negative. A 1% increase will decrease the 5-days registered cases by 100,000 people, by 319 and 32 respectively. An increase in the percentage of BME residents will increase the cases by 12. Looking at the temporal variable (‘by five days’), the model suggests that the cases would slightly increase across a 5 days window (0.17) in line with previous similar studies (Davies et al., 2021). Spatially, as the lens of analysis moves West, the cases will decrease and vice versa while moving South they should increase, however the latitude predictor is not statistically significant. Globally, only 3 out of 15 predictors are statistically insignificant. One limitation, compared to the GWR models is that the interpretation of the basis functions is less than straightforward and some of them are not statistically significant.

```{r, results = FALSE, warning=FALSE}
# transform current date in columns to rows
df_pivoted <- covid_19 %>%
   pivot_longer(
   cols = starts_with("X"),
   names_to = "date",
   names_prefix = "X",
   values_to = "cases",
   values_drop_na = TRUE
 )
```

```{r, results=FALSE}
# set new pivoted dataframe as sf dataframe
df_pivoted <- st_as_sf(df_pivoted, sf_column_name = "geom")
```
```{r, results=FALSE}
# parsing data into a time stamp
df_pivoted$date <- ymd(df_pivoted$date)
# check class of the newly created column
class(df_pivoted$date)
```

```{r}
# separate date variable into day,week, month and year variables
df_pivoted$day <- day(df_pivoted$date)     # days 
df_pivoted$week <- week(df_pivoted$date)   # week
df_pivoted$month <- month(df_pivoted$date) # months
df_pivoted$year <- year(df_pivoted$date)   # years
```
```{r}
# create a sequence of the 5 days time slots
serial_interval = seq(df_pivoted$date[1], tail(df_pivoted$date, n=1), by="5 days")
# set it as dataframe
serial_interval <- as.data.frame(serial_interval)
# create index to assign each '5 days' interval a by-one incremental slot
serial_interval <- serial_interval %>% mutate(index = seq(1, length(serial_interval), by=1))
```

```{r warning=FALSE, messages=FALSE}
# select variables to merge with new created dataframe
df_5days <- df_pivoted  %>%  dplyr::select(objectid,
                                             bng_n,
                                             long,
                                             lat,
                                             st_areasha,
                                             ctyua19cd, 
                                             ctyua19nm, 
                                             Region, 
                                             Crowded_housing,
                                             Work_from_home,                                                                              Mixed,Indian,Pakistani,Bangladeshi,
                                             Chinese,Other_Asian,Black,Other_ethnicity,
                                             Residents,
                                             Households,
                                             cases,
                                             date, day, week, month, year, long, lat)
```

```{r}
# create a new column with a 5 days time slots
df_5days <- df_5days %>% mutate(by_5_days=cut(date, "5 days"))
# transform in date format column
df_5days$by_5_days <- ymd(df_5days$by_5_days)
```

```{r}
# join with serial interval to get 5 days slots steps
df_5days <- right_join(df_5days, serial_interval, by = c("by_5_days" = "serial_interval"))
# add proportional values of the predictors
df_5days <- df_5days %>% mutate(
            crowded_hou_r = Crowded_housing / Households, # share of crowded housing
            work_from_home_r = (Work_from_home) / Residents, # share of resident working from home
            bme = (Mixed + Indian + Pakistani + Bangladeshi + Chinese + Other_Asian + Black + Other_ethnicity) / Residents, # share of black and minority ethnic residents
            )
```

```{r, message=FALSE, warning=FALSE}
# calculate cumulative cases by five days
df_5days <- df_5days %>% 
  group_by(index, ctyua19nm, crowded_hou_r, work_from_home_r, bme, Residents, by_5_days, date, long, lat, ctyua19cd, Region) %>% 
  summarise(cases_by_5days=sum(cases))
# calculate rate of infection for 5 days slots
df_5days <- df_5days %>% mutate(r_infections_by5 = (cases_by_5days / Residents) * 100000)
```

```{r eval=FALSE, include=FALSE}
tsp <- ggplot(data = df_5days,
            mapping = aes(x = date, y = cases_by_5days,
                          group = ctyua19nm))
tsp + geom_line(color = "blue") + 
    gghighlight(max(r_infections_by5) > 100, use_direct_label = FALSE) +
    labs(title= paste(" "), x="Date", y="Cumulative Cases per 100,000") +
    theme_classic() +
    theme(plot.title=element_text(size = 20)) +
    theme(axis.text=element_text(size=16)) +
    theme(axis.title.y = element_text(size = 18)) +
    theme(axis.title.x = element_text(size = 18)) +
    theme(plot.subtitle=element_text(size = 16)) +
    theme(axis.title=element_text(size=20, face="plain")) +
    facet_wrap(~ ctyua19nm)
```

```{r eval=FALSE, fig.height=100, fig.width=24, include=FALSE}
# map
legend_title = expression("Cases per 100,000 Population")
tm_shape(dplyr::filter(df_5days, index > 8)) + # filtering out Feb 2020
  tm_fill("r_infections_by5", title = legend_title, palette = magma(256), style ="cont", legend.hist=FALSE, legend.is.portrait=FALSE) +
  tm_facets(by = 'index', ncol = 3) +
  tm_borders(col = "white", lwd = .1)  + # add borders +
  tm_layout(bg.color = "white", # change background colour
            legend.outside = TRUE, # legend outside
            legend.outside.position = "bottom",
            legend.stack = "horizontal",
            legend.title.size = 2,
            legend.outside.size = 1,
            panel.label.size = 3,
            main.title = "COVID-19 Cases by 5 days, UTLA, England")
```



```{r echo=TRUE, fig.height=12, fig.width=12}
# create Hovmöller Plots and areas with residents > 300,000
ggplot(data = dplyr::filter(df_5days, Residents > 300000), 
            mapping = aes(x= date, y= reorder(ctyua19nm, r_infections_by5), fill= r_infections_by5)) +
            geom_tile() +
            scale_fill_viridis(name="New Cases per 100,000", option ="magma", begin = 0, end = 1, direction = 1) +
            theme_minimal() + 
            labs(title= paste("Fig. 3 - Infection Rate Across Major UTLAs"), x="Date", y="Upper Tier Authority Area") +
            theme(legend.position = "bottom")
            theme(legend.title = element_text(size=15)) +
            theme(axis.text.y = element_text(size=10)) +
            theme(axis.text.x = element_text(size=15)) +
            theme(axis.title=element_text(size=20, face="plain")) +
            theme(legend.key.width = unit(5, "cm"), legend.key.height = unit(2, "cm"))
```


```{r}
# create copy of covid_19_pivoted dataframe
covid_19_basis <- df_5days
# remove geometries
st_geometry(covid_19_basis) <- NULL
```

```{r results=FALSE}
# build basis functions
G <- auto_basis(data = covid_19_basis[,c("long","lat")] %>%
                       SpatialPoints(),           # To sp obj
                nres = 1,                         # One resolution
                type = "Gaussian")                # Gaussian BFs
# basis functions evaluated at data locations are then the covariates
S <- eval_basis(basis = G,                       # basis functions
                s = covid_19_basis[,c("long","lat")] %>%
                     as.matrix()) %>%            # conv. to matrix
     as.matrix()                                 # conv. to matrix
colnames(S) <- paste0("B", 1:ncol(S)) # assign column names
colnames(S) # check number of basis functions
```
```{r, message=FALSE, warning=FALSE}
# transform in S in dataframe
S <- as.data.frame(S)
# add basis functions (S) to the dataframe with variables
reg_df <- cbind(covid_19_basis, S) %>%
  dplyr::select(ctyua19nm, r_infections_by5, long, lat, by_5_days, work_from_home_r, crowded_hou_r, bme, B1:B9)
```

```{r, message=FALSE}
# fit linear regression
eq7 <- r_infections_by5 ~ long + lat + by_5_days + work_from_home_r + crowded_hou_r + bme + B1 + B2 + B3 + B4 + B5 + B6 + B7 + B8 + B9
m7 <- lm(formula = eq7, 
           data = dplyr::select(reg_df, -ctyua19nm))
```

```{r}
# create Table 2 with multiple models
export_summs(m2,m3,m4,m5,m7, model.names = c("Model 2","Model 3","Model 4","Model 5", "Model 7"))
```

## Conclusions
The study suggests how COVID-19 infections might change across geographical space. The ‘work from home’ policies seems particularly effective in reducing the spread, while a higher presence of people from BME groups suggest more cases probably because they are over disproportionally represented in the ‘essential workers’ category (eg. groceries, public transportation) and as result more subject to risk (The Health Foundation, 2020; Robinson et al., 2020). Less clear, it is the role of overcrowding houses especially since the existing literature suggests a positive relationship with the infection rate. The models might require further tuning to reduce the statistical insignificance of the variable. Further research explorations include studying across professions and areas how remote working might have impacted the infection rates, potentially suggesting areas of public policy improvements.

## Bibliography
+ Bartik, A. W., Cullen, Z. B., Glaeser, E. L., Luca, M. and Stanton, C. T. (2020) 'What Jobs are Being Done at Home During the Covid-19 Crisis? Evidence from Firm-Level Surveys', National Bureau of Economic Research Working Paper Series, No. 27422.
+ Belsley, David A, Edwin Kuh, and Roy E Welsch. 2005. Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. Vol. 571. John Wiley & Sons.)
+ Chang, S., Pierson, E., Koh, P. W., Gerardin, J., Redbird, B., Grusky, D. and Leskovec, J. (2021) 'Mobility network models of COVID-19 explain inequities and inform reopening', Nature, 589(7840), pp. 82-87.
+ Davies, N. G., Abbott, S., Barnard, R. C., Jarvis, C. I., Kucharski, A. J., Munday, J. D., Pearson, C. A. B., Russell, T. W., Tully, D. C., Washburne, A. D., Wenseleers, T., Gimma, A., Waites, W., Wong, K. L. M., van Zandvoort, K., Silverman, J. D., Diaz-Ordaz, K., Keogh, R., Eggo, R. M., Funk, S., Jit, M., Atkins, K. E. and Edmunds, W. J. (2021) 'Estimated transmissibility and impact of SARS-CoV-2 lineage B.1.1.7 in England', Science, 372(6538), pp. eabg3055.
+ Felstead, A and Reuschke, D (2020) ‘Homeworking in the UK: before and during the 2020 lockdown’, WISERD Report, Cardiff: Wales Institute of Social and Economic Research. Available for download from: https://wiserd.ac.uk/publications/homeworking-ukand-during-2020-lockdown
+ Green, M. A., García-Fiñana, M., Barr, B., Burnside, G., Cheyne, C. P., Hughes, D., Ashton, M., Sheard, S., Geddes, A., Rankin, J. and Buchan, I. E. (2021) 'Evaluating social and spatial inequalities of large scale rapid lateral flow SARS-CoV-2 antigen testing in COVID-19 management: An observational study of Liverpool, UK (November 2020 to January 2021)', medRxiv, pp. 2021.02.10.21251256.
+ Health  Foundation (2020) Black and minority ethnic workers make up a disproportionately large share of key worker sectors in London: The Health Foundation. Available at: https://www.health.org.uk/chart/black-and-minority-ethnic-workers-make-up-a-disproportionately-large-share-of-key-worker.
+ Rai, B., Shukla, A. and Dwivedi, L. K. (2021) 'Estimates of serial interval for COVID-19: A systematic review and meta-analysis', Clinical epidemiology and global health, 9, pp. 157-161.
+ Robinson, C., Rowe, F. and Patias, N. (2020) 'The Geography of the COVID-19 Pandemic in England'.
+ Tinson, A. and Clair, A. (2020) Better housing is crucial for our health and the COVID-19 recovery: The Health Foundation. Available at: https://www.health.org.uk/publications/long-reads/better-housing-is-crucial-for-our-health-and-the-covid-19-recovery#lf-section-110861-anchor.
